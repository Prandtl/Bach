\documentclass[12pt]{report}
\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{cite}
\usepackage[pdftex]{graphicx}

\textheight 24cm
\voffset -2cm
\textwidth 17cm
\hoffset -1.5cm
\topmargin 0mm

\renewcommand{\bottomfraction}{1.0}
  % Какая часть снизу листа может быть занята картинками
\renewcommand{\textfraction}{0.0}
  % Какая часть листа должна быть занята текстом

\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}
\newtheorem{ex}{Пример}
\newtheorem{tab}{Таблица}
\newtheorem{sat}{Утверждение}


\begin{document}
  \pagestyle{empty} % нумерация выкл.
  \begin{center}
    \small
    МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ РОССИЙСКОЙ ФЕДЕРАЦИИ\\
    Федеральное государственное автономное образовательное учреждение высшего образования\\
    УРАЛЬСКИЙ ФЕДЕРАЛЬНЫЙ УНИВЕРСИТЕТ \\
    имени первого Президента России Б.Н. Ельцина\\
    \vspace{1em}
    ИНСТИТУТ ЕСТЕСТВЕННЫХ НАУК И МАТЕМАТИКИ\\
    Кафедра математической экономики (TODO: Место выполнения ВКР и работы научного руководителя (консультанта) ВКР (либо кафедра, либо департамент))
  \end{center}

  \vspace{1em}

  \begin{center}
    \large
    АДАПТАЦИЯ АЛГОРИТМА ПАРАЛЛЕЛЬНОГО\\ СТОХАСТИЧЕСКОГО ГРАДИЕНТНОГО СПУСКА К ЗАДАЧАМ\\ МЕХАНИКИ СПЛОШНОЙ СРЕДЫ
    \vspace{1em}

    \normalsize
    Направление подготовки 09.03.03 <<Прикладная информатика>>\\
    \vspace{1em}
    Образовательная программа <<TODO: ???>>
    \end{center}
  \vspace{1em}

  \begin{tabular}[t]{@{}l}
    Допустить к защите:\\
    Директор департамента:\\ к.ф.-м.н., доц. М. О. Асанов\\
    \underline{\hspace{5cm}}\\
    \vspace{1em}\\
    Нормоконтроллер:\\ {TODO: find that person's name}\\
    \underline{\hspace{5cm}}
  \end{tabular}
  \hfill
  \begin{tabular}[t]{l@{}}
      Выпускная квалификационная\\работа бакалавра\\
      \textbf{Учанева}\\
      \textbf{Василия Вячеславовича}\\
      \underline{\hspace{5cm}}\\
      \vspace{1em}\\
      Научный руководитель:\\ к.ф.-м.н. В. С. Зверев\\
      \underline{\hspace{5cm}}\\
  \end{tabular}

  \vspace*{\fill}
  \begin{center}
    Екатеринбург 2017 г.
  \end{center}

  \newpage
  \pagestyle{plain} % нумерация вкл.
  \chapter*{Реферат}
  Учанев В. В. - -//- -: стр. - -,

  Ключевые слова: СТОХАСТИЧЕСКИЙ ГРАДИЕНТНЫЙ СПУСК, ПАРАЛЛЕЛИЗАЦИЯ.

  В данной работе рассматривается...{TODO:text}

  \newpage
  \tableofcontents

  \chapter{Введение}
  Оптимизация - поиск наилучшего элемента согласно некоторому критерию,
  например поиск максимального или минимального элемента. Для функции одной
  переменной существует аналитический метод поиска точек экстремума,
  но для функций двух и более переменных используются численные методы.

  Не смотря на то, что математические основы оптимизации были заложены
  в XVIII-XIX веке, широко использоваться они начали во второй половине XX века,
  в связи с появлением и распространением ЭВМ, поскольку использование
  методов оптимизации требует в большинстве случаев большой вычислительной работы.
  Поиск минимального/максимального значения используется во множестве областей:
  в экономике для оптимизации процессов и максимизации прибыли,
  в механике для проектирования и расчетов,
  в социальных науках для моделирования и оптимизации общественных процессов.

  За последние несколько лет был довольно большой скачок в развитии методов
  оптимизации, связанный c развитием машинного обучения. Методы оптимизации
  используются на этапе обучения: параметры модели подбираются так,
  чтобы уменьшить ошибку относительно тренировочного набора данных.

  Одним из наиболее используемых в машинном обучение алгоритмов является
  алгоритм градиентного спуска и его модификации. Причины этому в простоте его
  реализации и скорости работы, которая важна, так как во время обучения
  модель проходит через множество итераций и чем быстрее будет проходить
  каждая итерация, тем быстрее будет процесс поиска оптимальных параметров
  аппроксимирующей модели, то есть  обучение.

  \chapter{Постановка задачи}
  {TODO}

  \chapter{Градиентный спуск}
  \section{Градиентный спуск}

  Градиентный спуск - метод нахождения локального экстремума функции с помощью
  движения по направлению градиента/антиградиента. Это наиболее простой
  в реализации метод среди всех методов локальной оптимизации. Зачастую
  используется как часть других методов оптимизации. Этот метод не решает задачу
  поиска глобального минимума/максимума, то есть нужны модификации для поиска
  глобального экстремума.

  Если нам дана функция $F(x)$ и необходимо найти ее минимум, то шаг градиентного
  спуска выглядит следующим образом:
  \begin{equation}
    x^{j+1}=x^j- \lambda \nabla F(x^j)
  \end{equation}
  где $x^j$- приближение минимума функции на $j$-ом шаге, а $\lambda$ - шаг
  градиентного спуска, который может быть постоянной величиной или вычисляться
  на каждом шаге, например делением каждый раз на некое число, что увеличивает
  вероятность того, что алгоритм сойдется.
  Рассмотрим алгоритм градиентного спуска:
  \begin{enumerate}
    \item
    Задается начальное приближение: $x^0$ и необходимая точность расчета: $\alpha$.
    \item
    Рассчитывается по формуле 3.1 следующее приближение $x^{j+1}$.
    \item
    Проверяются условия остановки, например:
    \begin{enumerate}
      \item
      $|x^{j+1}-x^j| < \alpha$.
      \item
      $\exists i: x_i < x_{i_{min}}$ или $x_i > x_{i_{max}}$, где $x_{i_{min}}$ и $x_{i_{max}}$
      заранее заданые границы для координаты $x_i$.
      \item
      $j>j_max$, где $j_max$ - заранее заданное максимальное число итераций.
    \end{enumerate}
    Если условие остановки выполняется завершаем алгоритм, полученоу $x^j$ - наше найденое решение.
  \end{enumerate}

  \section{Проблемы алгоритма градиентного спуска}
  Градиентный спуск позволяет находить оптимальное решение в простых случаях, но
  в сложных задачах становятся видны следующие проблемы:
  \begin{enumerate}
    \item
    Для оптимальной работы алгоритма выжен правильно выбраный коэффициент $\lambda$
    - величина шага градиетного спуска. При слишком маленьком шаге алгоритм
    сходится слишком долго и может застрять в локальном минимуме, а при слишком
    большом шаге алгоритм будет перескакивать через решение и может расходиться.
    Возможным решением является, например, использование изменяющихся со временем
    значений для шага (т.н. Learning schedules). Но подобное решение будет
    сильно зависеть от данных и его придется адаптировать в каждом конкретном случае.
    \item
    Градиентный спуск застревает в локальных минимумах и в седловых точках.
    При этом седловые точки являются даже большей проблемой, так как чем больше
    размерность пространства в котором мы работаем, тем выше вероятность
    появления седловых точек.
    \item
    Градиентный спуск не оптимизирован для прохождения по оврагам - областям, где
    изменения по одной из компонент намного более крутые чем по другим.
    Градиентный спуск скачет с одной стороны оврага на другую, продвигаясь при
    этом на очень малое расстояние в сторону минимума.
  \end{enumerate}
  Чтобы решить эти проблемы градиентного спуска используются различные модификации,
  некоторые из которых мы рассмотрим далее.

  \section{Стохастический градиентный спуск}
  \section{Mini-batch gradient descent}
  \section{Градиентный спуск с импульсом}
  \section{Ускоренный градиентный спуск Нестерова}
  \section{Adagrad}
  \section{Adadelta и RMSProp}
  \section{ADAM}
  
  \chapter{Библиотека PETSc}
  {TODO}

  \chapter{Заключение}
  {TODO}

  \chapter{Список литературы}
  {TODO}

\end{document}
